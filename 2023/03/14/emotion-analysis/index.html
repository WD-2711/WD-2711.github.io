<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>emotion-analysis | wd-z711&#39;s B10g</title>
  <meta name="author" content="wd-z711">
  
  <meta name="description" content="Common student in China. Interested in web &amp; re.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="emotion-analysis"/>
  <meta property="og:site_name" content="wd-z711&#39;s B10g"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'G-YHJSKZDC3Y', 'auto');
  ga('send', 'pageview');
</script>




<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="wd-z711's B10g" type="application/atom+xml">
</head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">wd-z711&#39;s B10g</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> emotion-analysis</h1>
		</div>
	



<div class="row post">

	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <h1 id="情感分析-二分类"><a href="#情感分析-二分类" class="headerlink" title="情感分析-二分类"></a>情感分析-二分类</h1><h2 id="0x00-背景"><a href="#0x00-背景" class="headerlink" title="0x00 背景"></a>0x00 背景</h2><ul>
<li><p>任务数据集是<code>NLPCC14-SC</code>，<code>训练集:测试集=10000:2500</code>。<code>NVIDIA Tesla T4</code>相当于<code>NVIDIA GTX 750</code>。</p>
</li>
<li><p><code>transformers</code>库是一个用于自然语言处理（NLP）的 Python 第三方库，它提供了数千个预训练的模型，可以用于不同模态（如文本、图像、音频等）的任务。<code>transformers</code>库支持PyTorch深度学习框架，并且可以在它们之间无缝切换，<code>transformers</code>库还提供了简单的API和工具，可以快速下载和使用预训练的模型。</p>
</li>
<li><code>from google.colab import drive</code>的含义，这段代码的作用是从<code>google.colab</code>模块中导入<code>drive</code>对象，用于在colab虚拟机中挂载Google Drive云盘文件，以便访问云盘中的数据和代码。</li>
</ul>
<span id="more"></span>
<h2 id="0x01-Transformer与BERT"><a href="#0x01-Transformer与BERT" class="headerlink" title="0x01 Transformer与BERT"></a>0x01 Transformer与BERT</h2><p><strong>补充：</strong></p>
<ul>
<li><strong>自回归</strong>（Auto-regressive，AR）是一种常用于时间序列分析的方法。它通过<strong>对变量自身的历史值进行回归来预测未来的值</strong>。</li>
<li><strong>Minibatch</strong>是指一种训练方法，它将整个数据集分成若干个较小的批次（minibatch），<strong>每次训练时只使用一个批次的数据进行梯度下降</strong>。这种方法可以加快训练速度，并且可以通过调整批次大小和学习率来优化模型性能。</li>
<li><strong>注意力机制</strong>可以被描述为将一个查询（Query）和一组键-值对（Key-Value）映射到一个输出，其中查询、键、值和输出都是向量。输出计算为值（Value）的加权和，其中分配给每个值的权重由查询与相应键的相似度来计算。</li>
</ul>
<p><img src="/images/emotion-analysis/image-20230315185516313.png" alt="image-20230315185516313" style="zoom:67%;" /></p>
<ul>
<li><strong>BatchNorm与LayerNorm</strong>。BatchNorm是对一个batch中所有样本的同一维度特征进行归一化。而LayerNorm是对单个样本的所有维度特征进行归一化。<strong>BN一般使用在CV中，LN一般使用在NLP中</strong>，具体机理也不是特别清楚。</li>
<li><strong>预训练。</strong>指的是使用大量数据对深度学习模型进行预先训练，以执行特定任务（例如，识别图片中的分类问题）。预训练的目的是从<strong>大量数据中提取出尽可能多的共性特征</strong>，从而能让模型<strong>对特定任务的学习负担</strong>变轻。</li>
<li><strong>为什么注意力机制的计算开销与输入序列长度的平方成正比？</strong>这是因为注意力机制<strong>需要计算每个输入元素与其他所有输入元素之间的关系（相当于一个自相关函数）</strong>，因此计算量与输入序列长度的平方成正比。</li>
</ul>
<h3 id="语言模型概念"><a href="#语言模型概念" class="headerlink" title="语言模型概念"></a>语言模型概念</h3><p>&emsp;AutoRegressive语言模型：指的是依据前面（或后面）出现的单词来预测当前时刻的单词，代表有 ELMo， GPT等，优点是<strong>能够编故事</strong>，但只能利用单向语义。</p>
<p>&emsp;AutoEncoder语言模型：<strong>通过上下文信息来预测被mask的单词（让语言模型可以试图去还原其原始输入的系统）</strong>，代表有 BERT 。</p>
<h3 id="AutoEncoder-amp-Denoising-AutoEncoder-DAE"><a href="#AutoEncoder-amp-Denoising-AutoEncoder-DAE" class="headerlink" title="AutoEncoder&amp;Denoising AutoEncoder(DAE)"></a>AutoEncoder&amp;Denoising AutoEncoder(DAE)</h3><p>&emsp;AutoEncoder不需要标签，本质上是对数据的重构，但是当神经网络的参数复杂到一定程度时，AutoEncoder很容易存在过拟合的风险。为了缓解AutoEncoder过拟合的问题，一个办法是在输入中加入随机噪声，即DAE（Denoising AutoEncoder）。使用破损数据来训练，破损数据一定程度上<strong>减轻了训练数据与测试数据的代沟</strong>。由于数据的部分被擦掉了，因而这破损数据一定程度上比较接近测试数据。</p>
<h3 id="DAE与BERT模型的关系"><a href="#DAE与BERT模型的关系" class="headerlink" title="DAE与BERT模型的关系"></a>DAE与BERT模型的关系</h3><ul>
<li>BERT模型是基于<strong>Transformer Encoder</strong>来构建的一种模型，BERT模型基于<strong>DAE</strong>(Denoising AutoEncoder)的，这部分在BERT中被称为<strong>Masked Language Model (MLM)</strong>。</li>
<li>MLM并不是严格意义上的语言模型，它仅仅是训练语言模型的一种方式。BERT随机把一些单词通过<strong>MASK标签</strong>来代替，并接着去预测被<strong>MASK</strong>的这个单词，过程其实就是DAE的过程。</li>
</ul>
<h3 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h3><p>&emsp;Seq2seq模型，通常架构是：<code>Encoder+Attention（建立关联性）+Decoder</code>，可以使用CNN（卷积神经网络）、RNN、Transformer来做。</p>
<p>&emsp;CNN的特点有：<code>（1）权重共享（平移不变性、可并行计算）。（2）滑动窗口（局部关联性建模）（3）对相对位置敏感，对绝对位置不敏感。</code>每一层卷积只对局部范围建模，使用多层卷积，就可以进行<strong>长程建模</strong>。</p>
<p>&emsp;RNN依次有序的进行递归建模，其特点有：<code>（1）对顺序敏感。（2）串行计算耗时（可以通过堆RNN的深度来改善）。（3）长程建模能力弱。（4）对相对位置敏感，对绝对位置也敏感。</code></p>
<p>&emsp;Transformer的特点：<code>（1）无局部假设（CNN），即没有局部建模这一说，因此对相对位置不敏感，且可以并行计算。（2）无有序假设（RNN），对绝对位置不敏感，需要增加位置编码来反映位置变化对特征的影响。（3）任意两个字符都可以建模（邻近or离得远），擅长长短程建模，需要自注意力机制（复杂度比较高，为序列的平方）。</code></p>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p><img src="/images/emotion-analysis/image-20230315174522070.png" alt="image-20230315174522070" style="zoom: 50%;" /></p>
<p>&emsp;下面详细叙述该Transformer结构图。左边是Encoder部分，右边是Decoder部分。Encoder的输入是某个字符，输出是内部状态向量；Decoder的输入是上一时刻的字符与Encoder的内部状态向量，输出是字符的预测概率。</p>
<p><img src="/images/emotion-analysis/image-20230315180216805.png" alt="image-20230315180216805" style="zoom: 50%;" /></p>
<p><img src="/images/emotion-analysis/image-20230315181030203.png" alt="image-20230315181030203" style="zoom: 50%;" /></p>
<p><img src="/images/emotion-analysis/image-20230315181315698.png" alt="image-20230315181315698" style="zoom: 50%;" /></p>
<h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p>&emsp;BERT的全称是Bidirectional Encoder Representation from Transformers，模型是<strong>基于Transformer中的Encoder并加上双向的结构</strong>。BERT模型的<strong>主要创新点都在pre-train方法上，即用了Masked Language Model和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation（表示）</strong>。</p>
<p>&emsp;<strong>Masked Language Model</strong>指的是训练BERT的时候，随机把语料库中15%的单词做Mask操作。对于这15%的单词做Mask操作分为三种情况：80%的单词直接用[Mask]替换、10%的单词直接替换成另一个新的单词、10%的单词保持不变。</p>
<p>&emsp;由于BERT可能做Question Answering (QA：问题回答) 和 Natural Language Inference (NLI：翻译)之类的任务，因此增加了<strong>Next Sentence Prediction</strong>预训练任务，目的是让模型理解两个句子之间的联系。</p>
<p><img src="/images/emotion-analysis/image-20230315194938265.png" alt="image-20230315194938265" style="zoom:67%;" /></p>
<p>&emsp;BERT的作用：（1）情感分类（2）意图识别（3）问答匹配</p>
<h2 id="0x02-基于BERT的文本分类"><a href="#0x02-基于BERT的文本分类" class="headerlink" title="0x02 基于BERT的文本分类"></a>0x02 基于BERT的文本分类</h2><p><strong>引用1：</strong><a target="_blank" rel="noopener" href="https://www.ylkz.life/deeplearning/p10979382/">https://www.ylkz.life/deeplearning/p10979382/</a></p>
<p><strong>引用2：</strong><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/2136055">https://cloud.tencent.com/developer/article/2136055</a></p>
<p><strong>PS：月来客栈写的教程是真的好，很注重细节。</strong></p>
<p>&emsp;已知，BERT是一个强大的预训练模型，它可以基于谷歌发布的预训练参数<strong>在各个下游任务中进行微调</strong>。因此，在本篇文章中，掌柜将会介绍第一个下游微调场景，即如何在文本分类场景中基于BERT预训练模型进行微调。总的来说，<strong>基于BERT的文本分类模型就是在原始的BERT模型后再加上一个分类层即可。</strong>数据的分类层处理过程如下：</p>
<p><img src="/images/emotion-analysis/image-20230315202246130.png" alt="image-20230315202246130" style="zoom: 50%;" /></p>
<p>（1）需要将原始的数据样本进行分字（tokenize）处理；</p>
<p>（2）根据tokenize后的结果构造一个字典，不过在使用BERT预训练时并不需要我们自己来构造这个字典，直接载入谷歌开源的<code>vocab.txt</code>文件构造字典即可，因为只有<code>vocab.txt</code>中每个字的索引顺序才与开源模型中每个字的Embedding向量一一对应。</p>
<p>（3）根据字典将tokenize后的文本序列转换为Token序列，同时在Token序列的首尾分别加上<code>[CLS]</code>和<code>[SEP]</code>符号，并进行Padding。</p>
<p>（4）根据第3步处理后的结果生成对应的Padding Mask向量。</p>
<p>（5）最后，在模型训练时只需要将第3步和第4步处理后的结果一起喂给模型即可。</p>
<h2 id="0x03-K折交叉验证"><a href="#0x03-K折交叉验证" class="headerlink" title="0x03 K折交叉验证"></a>0x03 K折交叉验证</h2><h3 id="holdout-方法"><a href="#holdout-方法" class="headerlink" title="holdout 方法"></a>holdout 方法</h3><p>&emsp;holdout方法是将原始训练集分为三部分：训练集、验证集和测试集。训练集用于训练不同参数的模型，验证集用于模型选择。而测试集由于在训练模型和模型选择这两步都没有用到，对于模型来说是未知数据，因此可以用于评估模型的泛化能力。下图展示了holdout方法的步骤：</p>
<p><img src="/images/emotion-analysis/image-20230320204630310.png" alt="image-20230320204630310" style="zoom: 50%;" /></p>
<p>&emsp;但holdout方法也有明显的缺点，它对数据分割的方式很敏感，<strong>如果原始数据集分割不当，这包括训练集、验证集和测试集的样本数比例，以及分割后数据的分布情况是否和原始数据集分布情况相同等等</strong>。所以，不同的分割方式可能得到不同的最优模型参数。</p>
<h3 id="k折交叉验证"><a href="#k折交叉验证" class="headerlink" title="k折交叉验证"></a>k折交叉验证</h3><p>&emsp;k折交叉验证如下：</p>
<p>（1）使用不重复抽样将原始数据随机分为k份。</p>
<p>（2）k-1份数据用于模型训练，剩下1份数据用于测试模型。</p>
<p>（3）重复第2步k次，我们就得到了k个模型和其评估结果。</p>
<p>（4）将计算k折交叉验证结果的平均值作为参数/模型的性能评估。</p>
<p>（5）一旦找到最优参数，要使用这组参数在原始数据集上训练模型，作为最终的模型。</p>
<p>&emsp;10折交叉验证如下：</p>
<p><img src="/images/emotion-analysis/image-20230320205225657.png" alt="image-20230320205225657" style="zoom: 50%;" /></p>
<h2 id="0x04-TextCNN"><a href="#0x04-TextCNN" class="headerlink" title="0x04 TextCNN"></a>0x04 TextCNN</h2><p>&emsp;文本序列中的n-gram信息指的是文本中连续的n个词。在NLP任务中，<strong>n-gram可以用来评估或预测一个句子是否合理，或者提供一些自动完成、拼写检查、语法检查等功能</strong>。CNN网络可以很有效的捕捉文本序列中的<strong>n-gram</strong>信息，而<strong>分类任务从本质上讲是捕捉n-gram排列组合特征</strong>。无论是关键词、内容，还是句子的上层语义，在句子中均是以n-gram特征的形式存在的。</p>
<p>&emsp;<strong>Bert往往可以对一些表述隐晦的句子进行更好的分类，TextCNN往往对关键词更加敏感</strong>。</p>
<p>&emsp;Bert除去第一层输入层，有12个encoder层。每个encoder层的第一个token（CLS）向量都可以当作句子向量。我们可以理解为：encoder层越浅，句子向量越能代表低级别语义信息；越深，代表更高级别语义信息。</p>
<p>&emsp;如果我们既想得到有关词的特征，又想得到语义特征，模型具体做法是将第1层到第12层的CLS向量，作为CNN的输入然后进行分类。如下图所示：</p>
<p><img src="/images/emotion-analysis/image-20230321133820884.png" alt="image-20230321133820884" style="zoom: 50%;" /></p>
<h2 id="0x05-结果"><a href="#0x05-结果" class="headerlink" title="0x05 结果"></a>0x05 结果</h2><p>&emsp;最优结果<code>0.66</code>，绝对不是模型问题，应该调一调参会好很多，目前已知的调参可以达到<code>0.79</code>。</p>
<h2 id="0x06-另一种方法"><a href="#0x06-另一种方法" class="headerlink" title="0x06 另一种方法"></a>0x06 另一种方法</h2><p>&emsp;<strong>分层抽样：</strong>一种数据采样的方法，它根据不同群体的比例在数据里<strong>通过比例进行采样</strong>，减少采样的误差。分层抽样的目的是保证抽样后的数据集能够反映总体的特征，提高模型的泛化能力。最终结果可以到达<code>0.83</code>。</p>

  
	</div>

	<div>
  	<center>
	<div class="pagination">

    
    
    <a href="/2023/03/17/matrix-analysis-note/" type="button" class="btn btn-default"><i
                class="fa fa-arrow-circle-o-left"></i> 上一页</a>
    

    <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/2023/03/14/hvv-question/" type="button" class="btn btn-default ">下一页<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>

    </center>
	</div>


	<!-- comment -->
	
<section id="comment">
    <h2 class="title">留言</h2>

    <!-- 
    <div id="disqus_thread" class="ds-thread">
        <script type="text/javascript">
            /**
             * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
             * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
             */
                /*
                 var disqus_config = function () {
                 this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
                 this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
                 };
                 */
            (function() { // DON'T EDIT BELOW THIS LINE
                var d = document, s = d.createElement('script');

                s.src = 'https://https-wd-2711-tech.disqus.com/embed.js';

                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by
                Disqus.</a></noscript>
    </div> -->
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
    <div id="vcomments"></div>
    <script>
        new Valine({
            el: '#vcomments',
            appId: 'wLIJ70s3fPvW8WJPgVVAuWVR-gzGzoHsz',
            appKey: 'NVGqxhUkqtFv2CbwmXXWjCdR'
        })
    </script>

    
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2023-03-14 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/ML/">ML<span>7</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
		

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>

	<!-- copyright -->
	<div>
    <ul class="post-copyright">
      <li class="post-copyright-author">
      <strong>作者:  </strong>wd-z711</a>
      </li>
      <li class="post-copyright-link">
      <strong>文章链接:  </strong>
      <a href="/" target="_blank" title="">https://wd-2711.tech/</a>
      </li>
      <li class="post-copyright-license">
        <strong>版权声明:   </strong>
        本博客所有文章除特别声明外，均采用 <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
        许可协议。转载请注明出处!
      </li>
    </ul>
  <div>
 	

		


</div><!-- row -->




	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2024 wd-z711
  
      <!-- with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.     -->

     <!-- <script type="text/javascript">
      (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
      (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
      e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
      })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');
      
      _st('install','aB-vgeB8gEDrnRJuhceL','2.0.0');
    </script> -->
    </p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
   </html>

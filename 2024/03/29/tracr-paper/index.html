<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>tracr-paper | wd-z711&#39;s B10g</title>
  <meta name="author" content="wd-z711">
  
  <meta name="description" content="Common student in China. Interested in web &amp; re.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="tracr-paper"/>
  <meta property="og:site_name" content="wd-z711&#39;s B10g"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'G-YHJSKZDC3Y', 'auto');
  ga('send', 'pageview');
</script>




<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="wd-z711's B10g" type="application/atom+xml">
</head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">wd-z711&#39;s B10g</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> tracr-paper</h1>
		</div>
	



<div class="row post">

	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <h1 id="Tracr-Compiled-Transformers-as-a-Laboratory-for-Interpretability"><a href="#Tracr-Compiled-Transformers-as-a-Laboratory-for-Interpretability" class="headerlink" title="Tracr: Compiled Transformers as a Laboratory for Interpretability"></a>Tracr: Compiled Transformers as a Laboratory for Interpretability</h1><p>&emsp;编译器 Tracr 将以 RASP 编写的人类可读程序转换为标准的 decoder-only transformers。</p>
<h2 id="Transformer-架构与-RASP-编程语言"><a href="#Transformer-架构与-RASP-编程语言" class="headerlink" title="Transformer 架构与 RASP 编程语言"></a>Transformer 架构与 RASP 编程语言</h2><h3 id="Transformer-大体结构"><a href="#Transformer-大体结构" class="headerlink" title="Transformer 大体结构"></a>Transformer 大体结构</h3><p>&emsp;<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338817680">link</a>。</p>
<p>&emsp;之前自己学过 transformer，现在复习一下。Transformer 总体结构如下：</p>
<span id="more"></span>
<p><img src="/images/tracr-paper/image-20240329190814195.png" alt="image-20240329190814195" style="zoom:67%;" /></p>
<p>&emsp;Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：</p>
<p>Step1：获取输入句子的每一个单词的表示向量 X，X 由单词的 Embedding 和单词位置的 Embedding 相加得到，Embedding 就是从原始数据提取出来的 Feature。</p>
<p><img src="/images/tracr-paper/image-20240329191207670.png" alt="image-20240329191207670" style="zoom:67%;" /></p>
<p>Step2：将得到的向量矩阵传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 C。Encoder block 输出的矩阵维度与输入完全一致。</p>
<p><img src="/images/tracr-paper/image-20240329191528281.png" alt="image-20240329191528281" style="zoom:67%;" /></p>
<p>Step3：将 Encoder 输出的编码信息矩阵 C 传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1-&gt;i 翻译下一个单词 i+1。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作遮盖住 i+1 之后的单词。</p>
<p><img src="/images/tracr-paper/image-20240329191724166.png" alt="image-20240329191724166" style="zoom:67%;" /></p>
<p>&emsp;下面重点说一下 Transformer 的自注意力机制。</p>
<p><img src="/images/tracr-paper/image-20240329191949702.png" alt="image-20240329191949702" style="zoom:67%;" /></p>
<p>&emsp;如上图所示，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为 Multi-Head Attention，是由多个 Self-Attention 组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention。</p>
<p>&emsp;Multi-Head Attention 上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。</p>
<h3 id="Self-Attention-结构"><a href="#Self-Attention-结构" class="headerlink" title="Self-Attention 结构"></a>Self-Attention 结构</h3><p><img src="/images/tracr-paper/image-20240329192345893.png" alt="image-20240329192345893" style="zoom:67%;" /></p>
<p>&emsp;上图是 Self-Attention 的结构，需要用到矩阵 Q(查询)、K(键值)、V(值)。在实际中，Self-Attention 接收的是句子输入或者上一个 Encoder block 的输出。而 Q、K、V 是通过 Self-Attention 的输入进行线性变换得到的，具体而言，是 X 乘 $W_Q$、$W_K$ 与 $W_V$。可以将上述结构总结成如下式子：$\mathrm{Attention}(Q,K,V)=\mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$，$d_k$是 $Q$、$K$ 矩阵的列数，其中 Q 也叫做 query，K 也叫做 key，V 也叫做 value。Softmax 使得每一个元素的范围都在 (0,1) 之间，并且所有元素的和为 1。</p>
<h3 id="Multi-Head-Attention-结构"><a href="#Multi-Head-Attention-结构" class="headerlink" title="Multi-Head Attention 结构"></a>Multi-Head Attention 结构</h3><p>&emsp;Multi-Head Attention 是由多个 Self-Attention 组合形成的。</p>
<p><img src="/images/tracr-paper/image-20240330212449335.png" alt="image-20240330212449335" style="zoom:67%;" /></p>
<p>&emsp;可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入 X 分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵 Z。举个例子，例如 h=8：</p>
<p><img src="/images/tracr-paper/image-20240330212816383.png" alt="image-20240330212816383" style="zoom:67%;" /></p>
<p>&emsp;得到 8 个输出矩阵 $Z_1$ 到 $Z_8$ 之后，Multi-Head Attention 将它们拼接在一起 （Concat），然后传入一个 Linear 层，得到 Multi-Head Attention 最终的输出 Z，Z 的维度与 X 的维度是一样的：</p>
<p><img src="/images/tracr-paper/image-20240330212927825.png" alt="image-20240330212927825" style="zoom:67%;" /></p>
<h3 id="RASP"><a href="#RASP" class="headerlink" title="RASP"></a>RASP</h3><p>&emsp;Transformer 在每个注意力层和 MLP 层都有残差连接。残差链接充当一种存储器，较早的层可以使用它来将信息传递到后面的层。</p>
<p>&emsp;限制访问序列处理语言（Restricted Access Sequence Processing Language，RASP）是一种序列处理语言，具有两种类型的变量：序列操作 (s-ops) 和选择器，以及两种类型的指令：elementwise，select-aggregate。</p>
<p>&emsp;序列操作。序列运算 (s-op) 表示求值期间的值序列。标记和索引是内置的原始 s-op，它们分别返回输入标记的序列。例如：tokens(“hello”) = [h, e, l, l, o]。</p>
<p>&emsp;Select-aggregate。此操作对应于 Transformer 中的注意力。例如：</p>
<p><img src="/images/tracr-paper/image-20240330220718887.png" alt="image-20240330220718887" style="zoom:67%;" /></p>
<p>&emsp;$p(x, y) = x &lt; y$，其中 x 来自 indices，y 来自 s-op [1, 0, 2]。</p>
<p>&emsp;Aggregate 将 selector 和 s-op 作为输入，并生成一个 s-op。</p>
<p><img src="/images/tracr-paper/image-20240330221337257.png" alt="image-20240330221337257" style="zoom:67%;" /></p>
<p>&emsp;Selector 对应于 transformer 中的 attention。Select-aggregate 相当于 Transformer 中的 attention head。</p>
<h2 id="Tracr-架构"><a href="#Tracr-架构" class="headerlink" title="Tracr 架构"></a>Tracr 架构</h2><p>&emsp;我们不允许 selectors 的布尔组合，强制残差流的带注释的分类或数字嵌入（enforce annotated categorical or numerical embeddings for the residual stream），并强制使用 BOS 为开头。</p>
<p>&emsp;如果 RASP 是高级语言，那么 Craft 就是汇编语言。 Tracr 编译的模型可以转换为任何标准 decoder-only transformer 模型的权重。Tracr 通过六个步骤将 RASP 转换为 transformer 权重：</p>
<p>Step1：构建 computational graph。跟踪整个程序以创建表示计算的有向图，该图具有表示 tokens 和 indices 的 source node 以及用于输出 s-op 的 sink node。RASP 程序中的每个操作都成为 computational graph 中的一个节点。</p>
<p>Step2：推断 s-op 输入和输出值。 对于每个 s-op，需要决定如何将其嵌入到残差流中。要使用分类编码，我们需要知道 s-op 可以采用哪些值。所有节点都有一组有限的输出值，因为输入词汇和上下文是有限的。我们遍历该图并用其可能的输出来注释每个节点（程序分析？），以确保我们找到 s-op 将采用的值的超集。</p>
<p>Step3：将 computational graph 中的每个节点并将其转换为 model block。Elementwise 为 MLP 块，select-aggregate 为 attention block。我们使用 MLP 和 attention block 来模拟任意函数，具有分类输入和输出的 MLP 充当查找表，具有数字输入和输出的 MLP 充当分段线性近似。对于注意力层，我们将 selector 转换为 $W<em>{QK}$，并将 aggregate 转换为 $W</em>{OV}$。</p>
<p>Step4：将组件分配给层。为了构建 Transformer 模型，我们需要将 computational graph 中的所有模型块分配给层。理想情况下，我们希望找到最小的模型来执行所需的计算。我们通常可以将其表述为具有几个约束的组合优化问题：Transformer 架构具有交替的注意力层和 MLP 层，并且所有相互依赖的计算都需要采用正确的顺序。出于范围原因，我们启发式地解决这个问题。首先，我们计算从输入到给定节点的最长路径。该路径长度是我们可以分配节点的层数的上限。然后，我们应用额外的启发式方法将层与可以并行计算的块组合起来，这种方法返回正确但有时不是最优的层分配。</p>
<p>Step5：构建模型。我们将残差流空间构造为所有模型组件的输入和输出空间的直接和。换句话说，我们将每个 s-op 嵌入到它自己的正交子空间中，该子空间被保留供其在整个网络中单独使用。现在，我们可以按照层分配确定的顺序遍历计算图，并将组件堆叠起来以获得 Craft 中表示的完整 transformer。</p>
<p>Step6. 组装权重矩阵。最后，我们将模型的 Craft 表示转化为具体的模型权重。首先，我们将并行 MLP 层合并为单个层，并将并行注意力头合并为单个层。在注意力层中，我们将 $W<em>{QK}$ 和 $W</em>{OV}$ 矩阵分解为单独的 $W_q$、$W_k$、$W_o$、$W_v$ 权重矩阵。最后，我们调整所有权重的形状并将它们连接到 transformer。</p>
<p><img src="/images/tracr-paper/image-20240330234521636.png" alt="image-20240330234521636" style="zoom:67%;" /></p>
<h2 id="Tracr-转-transformer-的例子"><a href="#Tracr-转-transformer-的例子" class="headerlink" title="Tracr 转 transformer 的例子"></a>Tracr 转 transformer 的例子</h2><p>&emsp;明天整一个 RASP 程序，转为模型之后的样子。</p>
<p>&emsp;首先给出一个 RASP 的程序代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// sort_unique</span><br><span class="line">smaller = rasp.Select(keys, keys, rasp.Comparison.LT).named(&quot;smaller&quot;)</span><br><span class="line">target_pos = rasp.SelectorWidth(smaller).named(&quot;target_pos&quot;)</span><br><span class="line">sel_new = rasp.Select(target_pos, rasp.indices, rasp.Comparison.EQ)</span><br><span class="line">return rasp.Aggregate(sel_new, vals).named(&quot;sort&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="/images/tracr-paper/image-20240331105205993.png" alt="image-20240331105205993" style="zoom:67%;" /></p>
<p>&emsp;其中 children 是和程序一一对应的。先正常分析一波：</p>
<ul>
<li><p>SelectorWidth 直接转换为由注意力层和 MLP。它使用 BOS 令牌作为值输入，导致注意力头计算 x = 1/(1 + w)，其中 w 是所需的选择器宽度输出。</p>
</li>
<li><p>Selector。此操作对应于 Transformer 中的注意力。</p>
</li>
<li>Select-aggregate 相当于 Transformer 中的 attention head。</li>
</ul>
<p>再来看此程序编译成 model 之后的样子：</p>
<p><img src="/images/tracr-paper/image-20240331110747611.png" alt="image-20240331110747611" style="zoom:67%;" /></p>
<p>&emsp;可以看到，一共有两层：layer1 是 attention + mlp，layer2 也是 attention + mlp。</p>

  
	</div>

	<div>
  	<center>
	<div class="pagination">

    
    
    <a href="/2024/04/01/ctf-guide-pwn-1/" type="button" class="btn btn-default"><i
                class="fa fa-arrow-circle-o-left"></i> 上一页</a>
    

    <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/2024/03/27/haiku-study/" type="button" class="btn btn-default ">下一页<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>

    </center>
	</div>


	<!-- comment -->
	
<section id="comment">
    <h2 class="title">留言</h2>

    <!-- 
    <div id="disqus_thread" class="ds-thread">
        <script type="text/javascript">
            /**
             * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
             * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
             */
                /*
                 var disqus_config = function () {
                 this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
                 this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
                 };
                 */
            (function() { // DON'T EDIT BELOW THIS LINE
                var d = document, s = d.createElement('script');

                s.src = 'https://https-wd-2711-tech.disqus.com/embed.js';

                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by
                Disqus.</a></noscript>
    </div> -->
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
    <div id="vcomments"></div>
    <script>
        new Valine({
            el: '#vcomments',
            appId: 'wLIJ70s3fPvW8WJPgVVAuWVR-gzGzoHsz',
            appKey: 'NVGqxhUkqtFv2CbwmXXWjCdR'
        })
    </script>

    
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2024-03-29 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/papers/">papers<span>12</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
		

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
		   <span class="toc-title">Contents</span>
			<ol class="toc-article"><li class="toc-article-item toc-article-level-1"><a class="toc-article-link" href="#Tracr-Compiled-Transformers-as-a-Laboratory-for-Interpretability"><span class="toc-article-text">Tracr: Compiled Transformers as a Laboratory for Interpretability</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Transformer-%E6%9E%B6%E6%9E%84%E4%B8%8E-RASP-%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80"><span class="toc-article-text">Transformer 架构与 RASP 编程语言</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Transformer-%E5%A4%A7%E4%BD%93%E7%BB%93%E6%9E%84"><span class="toc-article-text">Transformer 大体结构</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Self-Attention-%E7%BB%93%E6%9E%84"><span class="toc-article-text">Self-Attention 结构</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Multi-Head-Attention-%E7%BB%93%E6%9E%84"><span class="toc-article-text">Multi-Head Attention 结构</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#RASP"><span class="toc-article-text">RASP</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Tracr-%E6%9E%B6%E6%9E%84"><span class="toc-article-text">Tracr 架构</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#Tracr-%E8%BD%AC-transformer-%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-article-text">Tracr 转 transformer 的例子</span></a></li></ol></li></ol>
		</div>
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>

	<!-- copyright -->
	<div>
    <ul class="post-copyright">
      <li class="post-copyright-author">
      <strong>作者:  </strong>wd-z711</a>
      </li>
      <li class="post-copyright-link">
      <strong>文章链接:  </strong>
      <a href="/" target="_blank" title="">https://wd-2711.tech/</a>
      </li>
      <li class="post-copyright-license">
        <strong>版权声明:   </strong>
        本博客所有文章除特别声明外，均采用 <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a>
        许可协议。转载请注明出处!
      </li>
    </ul>
  <div>
 	

		


</div><!-- row -->




	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2024 wd-z711
  
      <!-- with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.     -->

     <!-- <script type="text/javascript">
      (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
      (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
      e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
      })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');
      
      _st('install','aB-vgeB8gEDrnRJuhceL','2.0.0');
    </script> -->
    </p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

</body>
   </html>
